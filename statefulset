####first we want to create dynamic storage and manage the local disk resources. 
###to do the same simply install the fillowing utilitly
kubectl get sc
###currenly there is no storage class 
###storage class in k8s are being used to dynamically create storage in the cloud.
##below mentioned utility will create a local storage class
kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.24/deploy/local-path-storage.yaml
##########
kubectl get sc
####this will show you the storage class whcih is being created
local-path   rancher.io/local-path   Delete          WaitForFirstConsumer   false                  3h1m

####statefulset
vi state.yml
#############################
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  selector:
    matchLabels:
      app: nginx # has to match .spec.template.metadata.labels
  serviceName: "nginx"
  replicas: 2 # by default is 1
  minReadySeconds: 10 # by default is 0
  template:
    metadata:
      labels:
        app: nginx # has to match .spec.selector.matchLabels
    spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: nginx
        image: registry.k8s.io/nginx-slim:0.8
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www-nginx
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: "my-storage-class"
      resources:
        requests:
          storage: 1Gi

#####apply this manifest
kubectl replace -f state.yml --force
####lets troubleshoot the statefulset
 889  kubectl get pod
  890  kubectl get statefulset
  891  kubectl describe statefulset web
###check the event log of pvc
 893  kubectl get pvc
  894  kubectl describe pvc nginx-www-web-0
####the volumeMount and Volume claim template need to match
####we will open the state.yml file
vi state.yml
################################
      name: web
        volumeMounts:
        - name: www ###this should match 
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www ###this should match
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: "my-storage-class"
      resources:
        requests:
          storage: 1Gi
#######apply it and see the error
kubectl apply -f state.yml
####editing statefulset is not allow only few things like replica can be modified
kubectl replace -f state.yml --force
###check the pod
kubectl get statefulset
kubectl get pod
###in statefulset due to some reason if the first pod do not come up the other pod will never come up
 899  kubectl get pod
  900  kubectl describe pod web-0
####to resolve the below mentioned error
 Warning  FailedScheduling  3m8s (x4 over 18m)  default-scheduler  0/2 nodes are available: pod has unbound immediate PersistentVolumeClaims. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling..
####the problem is of storaage class name
vi state.yml
#######################
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: "my-storage-class" ##change the storage class name
      resources:
        requests:
          storage: 1
#####how to get the storage class name
kubectl get sc
####copy the name and replace it
 volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: "local-path" ##this is the new name of storage class
      resources:
        requests:
          storage: 1Gi
#####after updating the storage class
###we will delete the statefulset and pvc which is mapped to wrong storage class
  919  kubectl delete statefulset web
  920  kubectl delete pvc www-web-0
##############################################apply the new statefulset
921  kubectl apply -f state.yml
###as per the replicacount it is going to create that many number of pvc
  922  kubectl get pvc
  923  kubectl get statefulset
  924  kubectl get pod
#########################increase the replicacount to 3 and verify the storage and pod
925  vi state.yml
  926  kubectl apply -f state.yml
  927  kubectl get statefulset
  928  kubectl get pod
  929  kubectl get pvc
#########################decrease the replica count to 2 it will delete the pod but the pvc will not be deleted
925  vi state.yml
  926  kubectl apply -f state.yml
  927  kubectl get statefulset
  928  kubectl get pod
  929  kubectl get pvc

